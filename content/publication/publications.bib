%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Shamsuddeen Muhammad at 2020-11-07 05:02:33 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{Abdulmumin:2020aa,
	Abstract = {Improving neural machine translation (NMT) models using the back-translations of the monolingual target data (synthetic parallel data) is currently the state-of-the-art approach for training improved translation systems. The quality of the backward system - which is trained on the available parallel data and used for the back-translation - has been shown in many studies to affect the performance of the final NMT model. In low resource conditions, the available parallel data is usually not enough to train a backward model that can produce the qualitative synthetic data needed to train a standard translation model. This work proposes a self-training strategy where the output of the backward model is used to improve the model itself through the forward translation technique. The technique was shown to improve baseline low resource IWSLT'14 English-German and IWSLT'15 English-Vietnamese backward translation models by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the improved English-German backward model was used to train a forward model which out-performed another forward model trained using standard back-translation by 2.7 BLEU.},
	Author = {Idris Abdulmumin and Bashir Shehu Galadanci and Abubakar Isa},
	Date-Added = {2020-11-07 05:02:01 +0000},
	Date-Modified = {2020-11-07 05:02:01 +0000},
	Eprint = {2006.02876},
	Month = {06},
	Title = {Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation},
	Url = {https://arxiv.org/pdf/2006.02876.pdf},
	Year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/pdf/2006.02876.pdf},
	Bdsk-Url-2 = {https://arxiv.org/abs/2006.02876}}

@article{Abdulmumin:2019ab,
	Abstract = {An effective method to generate a large number of parallel sentences for training improved neural machine translation (NMT) systems is the use of back-translations of the target-side monolingual data. The method was not able to utilize the available huge amount of monolingual data because of the inability of models to differentiate between the authentic and synthetic parallel data. Tagging, or using gates, has been used to enable translation models to distinguish between synthetic and authentic data, improving standard back-translation and also enabling the use of iterative back-translation on language pairs that under-performed using standard back-translation. This work presents pre-training and fine-tuning as a simplified but more effective approach of differentiating between the two data. The approach - tag-less back-translation - trains the model on the synthetic data and fine-tunes it on the authentic data. Experiments have shown the approach to outperform the baseline and standard back-translation by 4.0 and 0.7 BLEU respectively on low resource English-Vietnamese NMT. While the need for tagging (noising) the dataset has been removed, the technique outperformed tagged back-translation by 0.4 BLEU. The approach reached the best scores in less training time than the standard and tagged back-translation approaches.},
	Author = {Idris Abdulmumin and Bashir Shehu Galadanci and Aliyu Garba},
	Date-Added = {2020-11-07 05:01:08 +0000},
	Date-Modified = {2020-11-07 05:01:08 +0000},
	Eprint = {1912.10514},
	Month = {12},
	Title = {Tag-less Back-Translation},
	Url = {https://arxiv.org/pdf/1912.10514.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1912.10514.pdf},
	Bdsk-Url-2 = {https://arxiv.org/abs/1912.10514}}

@article{Abdulmumin:2019aa,
	Abstract = {Words embedding (distributed word vector representations) have become an essential component of many natural language processing (NLP) tasks such as machine translation, sentiment analysis, word analogy, named entity recognition and word similarity. Despite this, the only work that provides word vectors for Hausa language is that of Bojanowski et al. [1] trained using fastText, consisting of only a few words vectors. This work presents words embedding models using Word2Vec's Continuous Bag of Words (CBoW) and Skip Gram (SG) models. The models, hauWE (Hausa Words Embedding), are bigger and better than the only previous model, making them more useful in NLP tasks. To compare the models, they were used to predict the 10 most similar words to 30 randomly selected Hausa words. hauWE CBoW's 88.7% and hauWE SG's 79.3% prediction accuracy greatly outperformed Bojanowski et al. [1]'s 22.3%.},
	Author = {Idris Abdulmumin and Bashir Shehu Galadanci},
	Date-Added = {2020-11-07 05:00:32 +0000},
	Date-Modified = {2020-11-07 05:00:32 +0000},
	Doi = {10.1109/NigeriaComputConf45974.2019.8949674},
	Eprint = {1911.10708},
	Month = {11},
	Title = {hauWE: Hausa Words Embedding for Natural Language Processing},
	Url = {https://arxiv.org/pdf/1911.10708.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1911.10708.pdf},
	Bdsk-Url-2 = {https://arxiv.org/abs/1911.10708},
	Bdsk-Url-3 = {https://doi.org/10.1109/NigeriaComputConf45974.2019.8949674}}

@article{ahmad2019review,
	Author = {Ahmad, Ibrahim Said and Bakar, Azuraliza Abu and Yaakub, Mohd Ridzwan},
	Journal = {International Journal of Advanced Computer Research},
	Number = {44},
	Pages = {283--292},
	Publisher = {International Journal of Advanced Computer Research},
	Title = {A review of feature selection in sentiment analysis using information gain and domain specific ontology},
	Volume = {9},
	Year = {2019}}

@article{ahmad2020survey,
	Author = {Ahmad, Ibrahim Said and Bakar, Azuraliza Abu and Yaakub, Mohd Ridzwan and Muhammad, Shamsuddeen Hassan},
	Journal = {SN Computer Science},
	Number = {4},
	Pages = {1--14},
	Publisher = {Springer},
	Title = {A survey on machine learning techniques in movie revenue prediction},
	Volume = {1},
	Year = {2020}}

@article{ahmadbeyond,
	Author = {Ahmad, Ibrahim Said and Bakar, Azuraliza Abu and Yaakub, Mohd Ridzwan and Darwich, Mohammad},
	Date-Modified = {2020-11-06 15:50:30 +0000},
	Journal = {International Journal of Advanced Computer Science and Applications(IJACSA)},
	Title = {Beyond Sentiment Classification: A Novel Approach for Utilizing Social Media Data for Business Intelligence},
	Volume = {3},
	Year = {2020}}

@article{osman2019current,
	Author = {OSMAN, AIDA and AHMAD, SAID},
	Journal = {Journal of Theoretical and Applied Information Technology},
	Number = {22},
	Title = {CURRENT TRENDS AND RESEARCH DIRECTIONS IN THE DICTIONARY-BASED APPROACH FOR SENTIMENT LEXICON GENERATION: A SURVEY},
	Volume = {97},
	Year = {2019}}

@article{ahmad2020sequel,
	Author = {Ahmad, Ibrahim Said and Bakar, Azuraliza Abu and Yaakub, Mohd Ridzwan and Darwich, Mohammad},
	Journal = {Data Technologies and Applications},
	Publisher = {Emerald Publishing Limited},
	Title = {Sequel movie revenue prediction model based on sentiment analysis},
	Year = {2020}}

@article{ahmad2020movie,
	Author = {Ahmad, Ibrahim Said and Bakar, Azuraliza Abu and Yaakub, Mohd Ridzwan},
	Journal = {Information Processing \& Management},
	Number = {5},
	Pages = {102278},
	Publisher = {Elsevier},
	Title = {Movie Revenue Prediction Based on Purchase Intention Mining Using YouTube Trailer Reviews},
	Volume = {57},
	Year = {2020}}

@article{nekoto2020participatory,
	Author = {Nekoto, Wilhelmina and Marivate, Vukosi and Matsila, Tshinondiwa and Fasubaa, Timi and Kolawole, Tajudeen and Fagbohungbe, Taiwo and Akinola, Solomon Oluwole and Muhammad, Shamsuddee Hassan and Kabongo, Salomon and Osei, Salomey and others},
	Journal = {arXiv preprint arXiv:2010.02353},
	Title = {Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages},
	Year = {2020}}

@inproceedings{muhammad2019overview,
	Author = {Muhammad, Shamsuddeen Hassan},
	Booktitle = {MAP-i Seminar Proceedings},
	Pages = {65--70},
	Title = {An overview of sentiment analysis approaches},
	Year = {2019}}

@inproceedings{muhammad2020incremental,
	Author = {Muhammad, Shamsuddeen Hassan and Brazdil, Pavel and Jorge, Al{\'\i}pio},
	Booktitle = {European Conference on Information Retrieval},
	Organization = {Springer},
	Pages = {619--623},
	Title = {Incremental Approach for Automatic Generation of Domain-Specific Sentiment Lexicon},
	Year = {2020}}

@inproceedings{bello2018reverse,
	Author = {Bello, Bello Shehu and Heckel, Reiko and Minku, Leandro},
	Booktitle = {2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
	Organization = {IEEE},
	Pages = {27--34},
	Title = {Reverse engineering the behaviour of twitter bots},
	Year = {2018}}

@article{inuwa2018lexical,
	Author = {Inuwa-Dutse, Isa and Bello, Bello Shehu and Korkontzelos, Ioannis},
	Journal = {arXiv preprint arXiv:1812.07947},
	Title = {Lexical analysis of automated accounts on Twitter},
	Year = {2018}}

@article{inuwa2018effect,
	Author = {Inuwa-Dutsea, Isa and Bello, Bello Shehu and Korkontzelos, Ioannis and Reiko, Heckel},
	Journal = {IADIS International Journal on WWW/Internet},
	Number = {2},
	Title = {The effect of engagement intensity and lexical richness in identifying bot accounts on twitter},
	Volume = {16},
	Year = {2018}}

@inproceedings{bello2019analyzing,
	Author = {Bello, Bello Shehu and Heckel, Reiko},
	Booktitle = {2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
	Organization = {IEEE},
	Pages = {61--66},
	Title = {Analyzing the Behaviour of Twitter Bots in Post Brexit Politics},
	Year = {2019}}
